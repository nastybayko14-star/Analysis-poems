import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Any

import file_utils
import text_utils


def analyze_single_text(filepath: str, filename: str = None) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª.
    
    Args:
        filepath (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        filename (str, optional): –ò–º—è —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ filepath)
        
    Returns:
        Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    if filename is None:
        filename = os.path.basename(filepath)
    
    print(f"  üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é: {filename}")
    
    # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
    content = file_utils.read_text_file(filepath)
    
    if content is None:
        print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª")
        return {}
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
    results = {
        "filename": filename,
        "word_count": text_utils.count_words(content),
        "unique_words": text_utils.count_unique_words(content),
        "ttr": text_utils.calculate_ttr(content),
        "line_count": text_utils.count_lines(content),
        "avg_word_length": text_utils.average_word_length(content),
        "longest_word": text_utils.find_longest_word(content),
        "lexical_density": text_utils.calculate_lexical_density(content),
        "file_size": os.path.getsize(filepath) if os.path.exists(filepath) else 0,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    readability = text_utils.analyze_text_readability(content)
    results.update(readability)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤
    most_common = text_utils.get_most_common_words(content, 3)
    results["top_words"] = ", ".join([f"{word}({count})" for word, count in most_common])
    
    print(f"    ‚úì –°–ª–æ–≤: {results['word_count']}, –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {results['unique_words']}, TTR: {results['ttr']:.3f}")
    
    return results


def analyze_corpus(corpus_folder: str = "corpus") -> List[Dict[str, Any]]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ.
    
    Args:
        corpus_folder (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        
    Returns:
        List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"\n{'='*60}")
    print(f" –ù–ê–ß–ò–ù–ê–Æ –ê–ù–ê–õ–ò–ó –ö–û–†–ü–£–°–ê")
    print(f" –ü–∞–ø–∫–∞: {corpus_folder}")
    print(f"{'='*60}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    text_files = file_utils.get_files_in_folder(corpus_folder, ".txt")
    
    if not text_files:
        print(f" –í –ø–∞–ø–∫–µ '{corpus_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (.txt).")
        return []
    
    print(f" –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(text_files)}")
    print("-" * 60)
    
    results = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for i, filepath in enumerate(sorted(text_files), 1):
        filename = os.path.basename(filepath)
        print(f"[{i:2d}/{len(text_files):2d}]", end="")
        
        result = analyze_single_text(filepath, filename)
        if result:
            results.append(result)
    
    print(f"\n –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(results)}")
    
    return results


def load_metadata(metadata_path: str = "data/metadata.csv") -> Dict[str, Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞.
    
    Args:
        metadata_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        Dict[str, Dict]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –∫–ª—é—á - –∏–º—è —Ñ–∞–π–ª–∞
    """
    metadata = {}
    
    if os.path.exists(metadata_path):
        print(f"\n –ó–∞–≥—Ä—É–∂–∞—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ {metadata_path}")
        data = file_utils.read_csv_file(metadata_path)
        
        if data:
            for item in data:
                filename = item.get("filename", "")
                if filename:
                    metadata[filename] = item
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(metadata)}")
    else:
        print(f"\n –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_path}")
        print("   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª data/metadata.csv —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ–∫—Å—Ç–∞—Ö")
    
    return metadata


def enrich_results_with_metadata(results: List[Dict], metadata: Dict[str, Dict]) -> List[Dict]:
    """
    –û–±–æ–≥–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        results (List[Dict]): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        metadata (Dict[str, Dict]): –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        
    Returns:
        List[Dict]: –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    if not metadata:
        return results
    
    for result in results:
        filename = result.get("filename", "")
        if filename in metadata:
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
            result.update(metadata[filename])
    
    return results


def generate_report(results: List[Dict], corpus_name: str = "–¢–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å") -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞.
    
    Args:
        results (List[Dict]): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–æ–≤
        corpus_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞
        
    Returns:
        str: –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç
    """
    if not results:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞."
    
    report_lines = []
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
    report_lines.append("=" * 80)
    report_lines.append(f" –û–¢–ß–Å–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–û–†–ü–£–°–ê: {corpus_name}")
    report_lines.append("=" * 80)
    report_lines.append(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(results)}")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report_lines.append("\n" + "=" * 80)
    report_lines.append(" –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    report_lines.append("=" * 80)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    total_words = sum(r.get("word_count", 0) for r in results)
    total_unique_words = sum(r.get("unique_words", 0) for r in results)
    avg_ttr = sum(r.get("ttr", 0) for r in results) / len(results) if results else 0
    avg_word_length = sum(r.get("avg_word_length", 0) for r in results) / len(results) if results else 0
    
    report_lines.append(f"\n –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
    report_lines.append(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words:,}")
    report_lines.append(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_unique_words:,}")
    report_lines.append(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π TTR: {avg_ttr:.4f}")
    report_lines.append(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {avg_word_length:.2f} —Å–∏–º–≤.")
    
    # –ù–∞—Ö–æ–¥–∏–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    if results:
        max_words = max(results, key=lambda x: x.get("word_count", 0))
        min_words = min(results, key=lambda x: x.get("word_count", 0))
        max_ttr = max(results, key=lambda x: x.get("ttr", 0))
        min_ttr = min(results, key=lambda x: x.get("ttr", 0))
        
        report_lines.append(f"\n –†–µ–∫–æ—Ä–¥—ã:")
        report_lines.append(f"  ‚Ä¢ –°–∞–º—ã–π –æ–±—ä–µ–º–Ω—ã–π —Ç–µ–∫—Å—Ç: {max_words.get('filename')} ({max_words.get('word_count', 0)} —Å–ª–æ–≤)")
        report_lines.append(f"  ‚Ä¢ –°–∞–º—ã–π –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {min_words.get('filename')} ({min_words.get('word_count', 0)} —Å–ª–æ–≤)")
        report_lines.append(f"  ‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (TTR): {max_ttr.get('filename')} ({max_ttr.get('ttr', 0):.4f})")
        report_lines.append(f"  ‚Ä¢ –ù–∞–∏–º–µ–Ω—å—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (TTR): {min_ttr.get('filename')} ({min_ttr.get('ttr', 0):.4f})")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É
    report_lines.append("\n" + "=" * 80)
    report_lines.append(" –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú:")
    report_lines.append("=" * 80)
    
    for i, result in enumerate(results, 1):
        report_lines.append(f"\n{i:3d}.  {result.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        report_lines.append(f"    {'‚îÄ' * 70}")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if "title" in result:
            report_lines.append(f"     –ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('title', '')}")
        if "author" in result:
            report_lines.append(f"     –ê–≤—Ç–æ—Ä: {result.get('author', '')}")
        if "year" in result:
            report_lines.append(f"     –ì–æ–¥: {result.get('year', '')}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report_lines.append(f"     –°–ª–æ–≤: {result.get('word_count', 0)}")
        report_lines.append(f"     –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {result.get('unique_words', 0)}")
        report_lines.append(f"     TTR: {result.get('ttr', 0):.4f}")
        report_lines.append(f"     –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {result.get('avg_word_length', 0):.2f} —Å–∏–º–≤.")
        report_lines.append(f"     –°—Ç—Ä–æ–∫: {result.get('line_count', 0)}")
        
        if "readability_score" in result:
            score = result.get("readability_score", 0)
            level = "—Å–ª–æ–∂–Ω—ã–π" if score < 50 else "—Å—Ä–µ–¥–Ω–∏–π" if score < 70 else "–ø—Ä–æ—Å—Ç–æ–π"
            report_lines.append(f"     –£–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ—Å—Ç—å: {score:.1f}/100 ({level})")
        
        if "top_words" in result and result["top_words"]:
            report_lines.append(f"     –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞: {result.get('top_words', '')}")
    
    # –í—ã–≤–æ–¥—ã –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
    report_lines.append("\n" + "=" * 80)
    report_lines.append(" –í–´–í–û–î–´ –ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")
    report_lines.append("=" * 80)
    
    if results:
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        ttr_values = [r.get("ttr", 0) for r in results]
        avg_ttr = sum(ttr_values) / len(ttr_values)
        
        if avg_ttr > 0.7:
            diversity = "–≤—ã—Å–æ–∫–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ"
        elif avg_ttr > 0.5:
            diversity = "—Å—Ä–µ–¥–Ω–µ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ"
        else:
            diversity = "–Ω–∏–∑–∫–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ"
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤
        word_lengths = [r.get("avg_word_length", 0) for r in results]
        avg_word_len = sum(word_lengths) / len(word_lengths)
        
        if avg_word_len > 6:
            word_len_desc = "–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"
        elif avg_word_len > 4:
            word_len_desc = "—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤"
        else:
            word_len_desc = "–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞"
        
        report_lines.append(f"\n –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:")
        report_lines.append(f"  1. –ö–æ—Ä–ø—É—Å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç {diversity} (—Å—Ä–µ–¥–Ω–∏–π TTR: {avg_ttr:.3f})")
        report_lines.append(f"  2. –í —Ç–µ–∫—Å—Ç–∞—Ö {word_len_desc} (—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_word_len:.1f} —Å–∏–º–≤.)")
        report_lines.append(f"  3. –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç {min(r.get('word_count', 0) for r in results)} –¥–æ {max(r.get('word_count', 0) for r in results)} —Å–ª–æ–≤")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report_lines.append(f"\n –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:")
        report_lines.append(f"  1. –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        report_lines.append(f"  2. –°—Ä–∞–≤–Ω–∏—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ—Ä–ø—É—Å–∞–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏")
        report_lines.append(f"  3. –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –ø–æ –∞–≤—Ç–æ—Ä–∞–º/–∂–∞–Ω—Ä–∞–º –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
    
    report_lines.append("\n" + "=" * 80)
    report_lines.append(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 80)
    
    return "\n".join(report_lines)


def save_statistics_csv(results: List[Dict], output_path: str = "results/statistics.csv") -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ CSV —Ñ–∞–π–ª.
    
    Args:
        results (List[Dict]): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        output_path (str): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞
        
    Returns:
        bool: True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
    """
    if not results:
        print(" –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV")
        return False
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è CSV
    # –û—Å–Ω–æ–≤–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    basic_headers = ["filename", "word_count", "unique_words", "ttr"]
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
    additional_headers = []
    if results:
        # –ë–µ—Ä–µ–º –≤—Å–µ –∫–ª—é—á–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –∫—Ä–æ–º–µ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö
        sample_keys = list(results[0].keys())
        for key in sample_keys:
            if key not in basic_headers and key not in additional_headers:
                additional_headers.append(key)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    additional_headers.sort()
    headers = basic_headers + additional_headers
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    success = file_utils.write_csv_file(output_path, results, headers)
    
    if success:
        print(f" –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
        print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(headers)}, –ó–∞–ø–∏—Å–µ–π: {len(results)}")
    
    return success


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã - —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞.
    """
    print(" –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í–û–ì–û –ö–û–†–ü–£–°–ê")
    print("=" * 50)
    print(" –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:")
    print("  corpus/     - —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    print("  data/       - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (metadata.csv)")
    print("  results/    - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
    for folder in ["corpus", "data", "results"]:
        if not os.path.exists(folder):
            os.makedirs(folder, exist_ok=True)
            print(f"‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {folder}/")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    text_files = file_utils.get_files_in_folder("corpus", ".txt")
    
    if not text_files:
        print("\n  –í–ù–ò–ú–ê–ù–ò–ï: –ü–∞–ø–∫–∞ 'corpus/' –ø—É—Å—Ç–∞—è!")
        print("   –î–æ–±–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (.txt) –≤ –ø–∞–ø–∫—É 'corpus/'")
        print("   –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 20 —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    print(f"\n –ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(text_files)}")
    
    if len(text_files) < 5:
        print("  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    start_time = time.time()
    
    try:
        # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä–ø—É—Å
        results = analyze_corpus("corpus")
        
        if not results:
            print("\n –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã")
            return
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = load_metadata("data/metadata.csv")
        
        # 3. –û–±–æ–≥–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        results = enrich_results_with_metadata(results, metadata)
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ CSV
        print(f"\n –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        csv_saved = save_statistics_csv(results, "results/statistics.csv")
        
        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        corpus_name = "–ú–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å"
        if metadata and "author" in next(iter(metadata.values()), {}):
            first_author = next(iter(metadata.values()))["author"]
            corpus_name = f"–ö–æ—Ä–ø—É—Å –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π {first_author}"
        
        report = generate_report(results, corpus_name)
        report_saved = file_utils.write_text_file("results/report.txt", report)
        
        # 6. –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        elapsed_time = time.time() - start_time
        
        print(f"\n{'='*50}")
        print(" –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"{'='*50}")
        print(f" –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(results)}")
        print(f"  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.1f} —Å–µ–∫.")
        print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/':")
        if csv_saved:
            print(f"  ‚Ä¢ statistics.csv - —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏")
        if report_saved:
            print(f"  ‚Ä¢ report.txt - –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç")
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç—á–µ—Ç
        print(f"\n{'='*50}")
        view = input(" –ü–æ–∫–∞–∑–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç? (–¥–∞/–Ω–µ—Ç): ").strip().lower()
        
        if view in ['–¥–∞', '–¥', 'yes', 'y', '1']:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –æ—Ç—á–µ—Ç–∞
            print("\n" + "=" * 80)
            print(" –ö–†–ê–¢–ö–ê–Ø –í–´–ë–û–†–ö–ê –ò–ó –û–¢–ß–ï–¢–ê:")
            print("=" * 80)
            lines = report.split('\n')[:30]  # –ü–µ—Ä–≤—ã–µ 30 —Å—Ç—Ä–æ–∫
            print('\n'.join(lines))
            print("\n... (–ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ results/report.txt)")
        
        print(f"\n –ì–æ—Ç–æ–≤–æ!")
            
    except KeyboardInterrupt:
        print("\n\n  –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    except Exception as e:
        print(f"\n –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    main()